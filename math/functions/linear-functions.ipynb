{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365a4f02",
   "metadata": {},
   "source": [
    "# Linear Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0def48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest for classification\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Probabilities of the positive class\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# Adjusted function to return bootstrapped AUC scores for visualization\n",
    "def bootstrap_auc(y_true, y_probs, n_bootstraps=1000, random_state=42):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    bootstrapped_scores = []\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        # Generate random indices with replacement\n",
    "        indices = rng.randint(0, len(y_probs), len(y_probs))\n",
    "        # Calculate AUC on the bootstrapped sample\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # Prevent an error if the sample is not diverse enough for AUC calculation\n",
    "            continue\n",
    "        score = roc_auc_score(y_true[indices], y_probs[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "    \n",
    "    # Calculate the lower and upper percentile to form the confidence interval\n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    confidence_lower = np.percentile(sorted_scores, 2.5)\n",
    "    confidence_upper = np.percentile(sorted_scores, 97.5)\n",
    "    \n",
    "    return confidence_lower, confidence_upper, bootstrapped_scores\n",
    "\n",
    "# Calculate confidence intervals for the AUC score and get the bootstrapped AUC scores\n",
    "conf_int_lower, conf_int_upper, bootstrapped_scores = bootstrap_auc(y_test, y_probs)\n",
    "\n",
    "# Print the results\n",
    "print(f\"AUC score: {auc_score:.3f}\")\n",
    "print(f\"95% Confidence interval for the AUC score: [{conf_int_lower:.3f}, {conf_int_upper:.3f}]\")\n",
    "\n",
    "# Visualize the distribution of bootstrapped AUC scores\n",
    "plt.hist(bootstrapped_scores, bins=50, alpha=0.75)\n",
    "plt.axvline(auc_score, color='red', linestyle='--', label='AUC Score')\n",
    "plt.axvline(conf_int_lower, color='green', linestyle='--', label='2.5th percentile')\n",
    "plt.axvline(conf_int_upper, color='green', linestyle='--', label='97.5th percentile')\n",
    "plt.title('Distribution of Bootstrapped AUC Scores')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
